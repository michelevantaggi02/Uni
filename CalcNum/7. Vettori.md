$$w,v \in K^n$$

$$v=<v_1,...,v_n>$$

$$w = <w_1,...,w_n>$$

$$v + w = <v_1 + w_1,....,v_n + w_n>$$

# Prodotto Riga per Colonna Tra Due Vettori

$$v^t \cdot w = [v_1,...,v_n]\begin{bmatrix}
w_1 \\ \vdots\\w_n
\end{bmatrix} = v_1w_1 + v_2 w_2 +... + v_nw_n$$

# Prodotto Matrice per Vettore

$$A \in K^{mxn}$$
$$b \in K^n$$
$$Ab = c \in K^m$$
$$A = <r_i^t,...,r_m^t>$$

Faccio il prodotto riga per colonna tra le righe della matrice e il vettore.

Ci sono anche altre interpretazioni, come quello impiegato nel deep learning

# Prodotto Tra Matrici Di Strassen

Algoritmo per semplificare il prodotto tra due matrici 2x2.

La moltiplicazione riga per colonna impiega 8 moltiplicazioni e 4 addizioni.

L'algoritmo di Strassen impiega 7 moltiplicazioni e 18 addizioni.

Può sembrare che impieghi un tempo peggiore rispetto alla riga per colonna, ma la moltiplicazione tra due vettori impiega molto di più che l'addizione, rendendo quindi l'algoritmo più veloce.

L'algoritmo viene poi trasformato in moltiplicazioni in blocchi. Rendendo qualsiasi moltiplicazione tra matrici $2^nx2^n$  come 7 moltiplicazioni e alcune addizioni tra 4 matrici $2^{(n-1)}x2^{(n-1)}$ ricorsivamente.

Questo ha fatto scaturire una corsa all'algoritmo più veloce per risoluzione di prodotti tra matrici.

Ma questi algoritmi non vengono usati.

# Somme Di Elementi in Parallelo

Ho un vettore di $2^n$ elementi che devono essere sommati tra loro.

Potrei scorrere tutto il vettore e sommare il primo elemento con il secondo poi con il terzo e così via, però sarebbe troppo lento. Posso quindi sommare in parallelo gli elementi a due a due, creando così un secondo vettore di $2^{n-1}$ elementi e ripeto l'operazione finché non arrivo a un vettore di 1 elemento. Il risultato è lo stesso ma eseguendo le operazioni parallelamente con vettori molto grandi l'algoritmo è molto più veloce.