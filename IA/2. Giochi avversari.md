Search-non adversary:

- Ricerca euristica per trovare il goal
- Possiamo trovare la soluzione ottima
- Le funzioni di valutazione vengono determinate dal costo

Games-adversary:

- Strategia per ogni possibile mossa dell'avversario
- Il limite di tempo obbliga ad approssimare la soluzione
- Le funzioni di valutazione delle volte non sono ovvie

La difficoltà del gioco è determinata dal branching factor (il numero di mosse possibli in un certo livello).

Sappiamo che un gioco è __risolto__ quando la __soluzione è ottima__.

# Tipi Di Giochi

- deterministici o stocastici
- uno, due o più giocatori
- Somma zero
- informazioni perfette (puoi vedere lo stato)

Voglio un algoritmo che calcoli una strategia (policy) che consigli una mossa per ogni stato.

## Giochi Deterministici

Ci sono molte formalizzazioni possibili, una è:

- Stati: $S$ (parte da $s_0$)
- Giocatori $P= \{1,...,n\}$ (di solito a turni)
- Azioni: $A$ (dipendono dal giocatore / stato)
- Funzioni di transizione: $S\times A \to S$
- Test terminali: $S \to \{t,f\}$
- Utilità terminali: $S \times P \to R$

La soluzione per un giocatore è un insieme di azioni chiamata __policy__:$S \to A$

## Giochi a Somma Zero

- Gli agenti hanno utilità opposte
- Ci fa pensare ad un singolo valore che uno massimizza e un'altro minimizza
- Avversari, pura competizione

### Giochi Generali

- Gli agenti hanno utilità indipendenti
- Cooperazione, indifferenza, competizione e altro sono tutti possibili

# Algoritmo MiniMax

	function MINIMAX-SEARCH(game, state) returns an action
		player <- game.TO-MOVE(state)
		value, move <- MAX-VALUE(game, state)
		return move

	function MAX-VALUE(game, state) returns (utility, move)
		if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null
		v <- -INFINITE
		for each a in game.ACTIONS(state) do
			v2, a2 <- MIN-VALUE(game, game.RESULT(state, a))
			if v2 > v then
				v, move <- v2, a
		return v, move

	function MIN-VALUE(game, state) returns (utility, move)
		if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null
		v <- INFINITE
		for each a in game.ACTIONS(state) do
			v2, a2 <- MAX-VALUE(game, game.RESULT(state, a))
			if v2 < v then
				v, move <- v2, a
		return v, move

- Algoritmo di gioco completo
- utility(s) $\forall s$ terminale definito utility di max

$$
\max(s) = \begin{cases}

\text{utility}(s, \max)&&\text{if s è terminale}\\
\substack{\max \\ a \in \text{actions(s)}} & \text{minimax(results(s,a))}& \max=\text{to\_move(s)}\\
\substack{\min \\ a \in \text{actions(s)}} & \text{minimax(results(s,a))}& \min=\text{to\_move(s)}\\

\end{cases}
$$

Definizione problema gioco a 2 giocatori a somma 0

## Implementazione Minimax

	function max-value(state):
		v <- -INFINITE
		for each successor of state:
			v = max(v, min-value(successor))
		return v

	function min-value(state):
		v = INFINITE
		for each successor of state:
			v = min(v, max-value(successor))
		return v

![[imp_minimax.png|center big]]

```ad-example
title: Minimax

![[minimax.svg]]

Abbiamo gli stessi vantaggi della ricerca in profondità, quindi possiamo eliminare i nodi che non ci servono, con tempo esponenziale


![[minimax 1.svg]]

Una volta trovato il primo min (3), se troviamo un valore che sia inferiore possiamo ignorare tutti gli altri valori superiori.

Se stiamo calcolando il MIN-VALUE di un certo nodo $n$:

- Iteriamo tra i child di $n$
- La stima dei child sta calando
- A chi importa del valore di $n$? MAX
- Sia $a$ il valore migliore che MAX possa scegliere lungo il percorso corrente dalla radice
- Se $n$ diventa peggiore di $a$, MAX lo eviterà, così possiamo ignorare gli altri figli di $n$

Per MAX è la stessa cosa ma al contrario

```

## Alfa-Beta Pruning

	function ALFA-BETA-SEARCH(game, state) returns an action
		player <- game.TO-MOVE(state)
		value, move <- MAX-VALUE(game, state, -INFINITE, INFINITE)
		return move

	function MAX-VALUE(game, state, alfa, beta) returns (utility, move)
		if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null
		v <- -INFINITE
		for each a in game.ACTIONS(state) do
			v2, a2 <- MIN-VALUE(game, game.RESULT(state, a), alfa, beta)
			if v2 > v then
				v, move <- v2, a
				alfa <- MAX(alfa, v)
			if v >= beta then return v, move
		return v, move

	function MAX-VALUE(game, state, alfa, beta) returns (utility, move)
		if game.IS-TERMINAL(state) then return game.UTILITY(state, player), null
		v <- INFINITE
		for each a in game.ACTIONS(state) do
			v2, a2 <- MAX-VALUE(game, game.RESULT(state, a), alfa, beta)
			if v2 < v then
				v, move <- v2, a
				beta <- MIN(beta, v)
			if v <= alfa then return v, move
		return v, move

![[minimax 1.svg]]

- Il pruning non ha effetto sui valori calcolati per la radice
- I valori dei nodi intermedi potrebbero essere sbagliati
- Un buon ordinamento dei child migliora l'efficacia del pruning
- Con un ordinamento perfetto:
	- Complessità di tempo scende a $O(b^{m/2})$
	- Raddoppia la profondità risolvibile
	- La ricerca completa resta senza speranze (ad esempio a scacchi)

Questo è un esempio di __metareasoning__ (pensare a cosa calcolare)

# Limitare La Profondità

Realisticamente nei giochi non si può cercare tutto fino alle foglie, per questo viene suggerita la Depth-limited search.

- Invece di controllare tutto il grafo, si controlla soltanto fino a un certo limite di profondità
- Si rimpiazzano le utilities terminali con una __funzione di valutazione__ per le posizioni che non terminano.

```ad-example

Supponendo di avere 100 secondi per eseguire una mossa e che il nostro calcolatore può esplorare 10k nodi / secondo, possiamo esplorare 1 milione di nodi a mossa.

Con [[#Alfa-Beta pruning]] raggiungiamo all'incirca una profondità di 8, che in un programma di scacchi è una profondità decente.

```

## Funzioni Di Valutazione

Queste funzioni servono a dare un valore ai nodi non terminali che si raggiungono alla fine di una depth-limited search.

Idealmente dovrebbero ritornare l'effettivo valore minimax della posizione controllata, cosa che è effettivamente impossibile.

Nella pratica infatti si utilizza una somma di determinati valori (Es: la differenza tra il numero di pezzi dello stesso tipo) $$Eval(s) = w_1f_1(s) + ...+w_nf_n(s)$$

Queste funzioni sono __sempre imperfette__, più in profondità "nascondiamo" la funzione di valutazione, meno importante sarà la sua qualità.

# Risultati Incerti

Se vogliamo modellare elementi casuali che accadono nel mondo bisogna costruire algoritmi efficienti basati sulla casualità (Ex: Monte Carlo Tree Search), questi risultati incerti __non vengono controllati dall'avversario__, ma dalla __probabilità__

## Expectimax Search

```ad-question
title: Perché non possiamo sapere quale sarà il risultato di un'azione?

Potrebbe essere perché il gioco è basato sulla casualità (Es: tirare un dado), oppure l'avversario è imprevedibile (avversario che risponde casualmente), oppure alcune azioni possono fallire (Un robot che ha un malfunzionamento)

```

I valori in questo caso dovrebbero riflettere il __caso medio__ (expectimax), e non il caso peggiore (minimax)

```ad-important
title: Expectimax search

Calcola il valore meido secondo la giocata ottimale:

- Nodi MAX come in [[#Algoritmo MiniMax]]
- I nodi chance sono come i nodi MIN ma il risultato è incerto
- Calcola la loro utilità aspettata
- Prende la media pesata (aspettativa) dei figli

```

	function EXPECTIMAX-SEARCH(state)
		if IS-TERMINAL(state) then return state.utility
		else if NEXT-AGENT(state) = MAX then return MAX-VALUE(state)
		else if NEXT-AGENT(state) = MIN then return EXP-VALUE(state)

	function MAX-VALUE(state)
		v <- -INFINITE
		for each successor of state do
			v <- max(v, value(successor))
		return v

	function EXP-VALUE(state)
		v <- 0
		for each successor of state do
			p <- probability(successor)
			v += p * value(successor)
		return v

```ad-question
title: Quali probabilità usare?

Con la ricerca expectimax abbiamo un modello di probabilità di come l'avversario (o l'ambiente) si comporteranno in ogni stato. Il modello può essere sia semplice (come il tiro di un dado) che sofisticato (richiedenndo una grande quantità di calcoli).

Abbiamo un nodo probabilità per ogni risultato fuori dal nostro controllo: avversario o ambiente.

Il modello potrebbe anche dire che le azioni dell'avversario si assomigliano, per il momento bisogna assumere che ogni nodo probabilità venga fuori magicamente con delle probabilità che specificano la distribuzione sul suo risultato.

```

```ad-example
title: Probabilità informata

Diciamo che tu sappia che il tuo avversario stia utilizzando un minimax a profondità 2, utilizzando il suo risultato L'80% delle volte, mentre il 20% si muove casualmente.


In questo caso possiamo utilizzare expectimax per capire ogni probabilità dei nodi, eseguendo una simulazione del tuo avversario.

Questo tipo di cose diventano lente molto velocentente, ed è ancora peggio se devi simulare il tuo avversario che simula te che simuli tu ecc...

... Tranne per minimax, visto che viene tutto raggruppato in un singolo albero

```

# Mixed Layer

Un esempio è il backgammon, l'ambiente è un ulteriore "agente casuale" che si muove dopo ogni mossa di un agente min/max. Ogni nodo calcola l'appropriata combinazione dei suoi figli.

```ad-example
title: Backgammon

Il tiro di dadi aumenta $b: 21$ possibili tiri con 2 dadi.

A backgammon ci sono circa 20 mosse, con una profondità di 2 si ha $$\text{ Depth 2} = 20 \cdot (21 \cdot 21)^3 = 1.2 \cdot 10^9$$
Con l'aumentare della profondità, la probabilità che si raggiunga un dato nodo diminuisce, quindi è meno utile utilizzare la ricerca e limitare la profondità crea meno danni, ma è difficile il pruning.

TDGammon utilizza depth-2 search + funzione di valutazione molto buona + reinforcement learning + livello di gioco da campionato mondiale, risultato: 1a IA campione del mondo


```

# Utilità Multi-agente

Se il gioco non è a somma zero, oppure ha più giocatori, si può utilizzare una generalizzazione di minimax:

- I terminali hanno tuple di utilità
- I valori dei nodi sono anche loro tuple di utilità
- Ogni giocatore massimizza il proprio componente
- Può gestiire la cooperazione o la competizione dinamicamente

# Difficoltà con la ricerca

Anche utilizzando [[#Alfa-Beta Pruning]] e [[#Limitare La Profondità]], un $b$ molto grande crea problemi (ricorda che la complessità in tempo migliore è $b^{m/2}$).

Sugli scacchi è fattibile, con alfa-beta si ha $35^{(8/2)} \approx 1M$ , profondità 8 va abbasta bene;

Ma con GO: $300^{(8/21)} \approx 8B$ 

Inoltre limitare la profondità richiede di creare delle buone funzioni di valutazione __per ogni problema__, che se fatte male possono generare soluzioni inefficienti.